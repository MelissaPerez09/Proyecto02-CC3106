{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f16475",
   "metadata": {},
   "source": [
    "# Etapa 1 - Preprocesamiento\n",
    "Este notebook lee **Data/raw/**, aplica limpieza mínima, añade **VADER** y heurísticos, agrega métricas por post y guarda **CSV + JSONL** en **Data/processed/**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67544eb9",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d7e5196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar esta celda para instalar las siguientes dependencias:\n",
    "# ```bash\n",
    "# pip install pandas tqdm vaderSentiment regex\n",
    "# ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47cfa9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b25e87",
   "metadata": {},
   "source": [
    "## Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a72c614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")          # base de datos\n",
    "RAW_DIR = DATA_DIR / \"raw\"       # carpeta con la corrida cruda\n",
    "ENRICHED_DIR = DATA_DIR / \"enriched\"  # carpeta para datos enriquecidos\n",
    "ENRICHED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Heurísticos acuerdo/desacuerdo (EN/ES). Puedes ampliar listas.\n",
    "AGREE_TERMS = [\n",
    "    r\"\\bi agree\\b\", r\"\\bagree\\b\", r\"\\bsupported\\b\", r\"\\bsupport this\\b\", r\"\\bi support\\b\", r\"\\bvalid point\\b\",\n",
    "    r\"\\bde acuerdo\\b\", r\"\\bapoyo\\b\", r\"\\btiene raz[oó]n\\b\", r\"\\bcierto\\b\", r\"\\btotalmente de acuerdo\\b\", r\"\\bestoy de acuerdo\\b\",\n",
    "]\n",
    "DISAGREE_TERMS = [\n",
    "    r\"\\bi disagree\\b\", r\"\\bdisagree\\b\", r\"\\bnot support\\b\", r\"\\boppose\\b\", r\"\\bagainst this\\b\", r\"\\bbad take\\b\",\n",
    "    r\"\\bno apoyo\\b\", r\"\\ben desacuerdo\\b\", r\"\\bno estoy de acuerdo\\b\", r\"\\bmala idea\\b\", r\"\\bme opongo\\b\",\n",
    "]\n",
    "\n",
    "# Expresiones regulares precompiladas\n",
    "RE_URL = re.compile(r\"https?://\\S+\")\n",
    "RE_WS = re.compile(r\"\\s+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1f745",
   "metadata": {},
   "source": [
    "## Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e431bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_raw_dir(raw_base: Path) -> Path:\n",
    "    \"\"\"Obtiene la subcarpeta más reciente en raw_base.\"\"\"\n",
    "    subs = [p for p in raw_base.iterdir() if p.is_dir()]\n",
    "    if not subs:\n",
    "        raise FileNotFoundError(f\"No hay subcarpetas en {raw_base}. Asegura haber corrido el scraper.\")\n",
    "    return sorted(subs)[-1]\n",
    "\n",
    "\n",
    "def read_csv_or_jsonl(path_csv: Path, path_jsonl: Path) -> pd.DataFrame:\n",
    "    \"\"\"Lee un DataFrame desde CSV o JSONL, dependiendo de cuál exista.\"\"\"\n",
    "    if path_csv.exists():\n",
    "        return pd.read_csv(path_csv)\n",
    "    if path_jsonl.exists():\n",
    "        return pd.read_json(path_jsonl, lines=True)\n",
    "    raise FileNotFoundError(f\"No se encontró ni {path_csv} ni {path_jsonl}\")\n",
    "\n",
    "\n",
    "def minimal_clean_for_vader(text: str) -> str:\n",
    "    \"\"\"Limpieza mínima: normaliza espacios y opcionalmente tokeniza URLs.\n",
    "    Conserva mayúsculas, puntuación y emojis para VADER.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = RE_URL.sub(\"URL\", text)\n",
    "    t = RE_WS.sub(\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "def count_hits_regex(text: str, patterns: list[str]) -> int:\n",
    "    \"\"\"Cuenta cuántos patrones en 'patterns' hacen match en 'text' (case insensitive).\"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    t = text.lower()\n",
    "    return sum(1 for pat in patterns if re.search(pat, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae09e0",
   "metadata": {},
   "source": [
    "## Carga de datos crudos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0c1a026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes RAW: (200, 11) (15131, 6)\n"
     ]
    }
   ],
   "source": [
    "posts_csv = RAW_DIR / \"posts.csv\"\n",
    "comments_csv = RAW_DIR / \"comments.csv\"\n",
    "posts_jsonl = RAW_DIR / \"posts.jsonl\"\n",
    "comments_jsonl = RAW_DIR / \"comments.jsonl\"\n",
    "\n",
    "posts = read_csv_or_jsonl(posts_csv, posts_jsonl)\n",
    "comments = read_csv_or_jsonl(comments_csv, comments_jsonl)\n",
    "print(\"Shapes RAW:\", posts.shape, comments.shape)\n",
    "\n",
    "# Asegura columnas básicas\n",
    "req_posts = [\"post_id\",\"title\",\"selftext\",\"author\",\"created\",\"score\",\"num_comments\",\"permalink\",\"is_self\",\"image_urls\",\"subreddit\"]\n",
    "for col in req_posts:\n",
    "    if col not in posts.columns:\n",
    "        posts[col] = None\n",
    "\n",
    "req_comments = [\"post_id\",\"comment_id\",\"author\",\"created\",\"score\",\"body\"]\n",
    "for col in req_comments:\n",
    "    if col not in comments.columns:\n",
    "        comments[col] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd8ce1",
   "metadata": {},
   "source": [
    "## Limpieza mínima para VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a0860d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza mínima y features en posts\n",
    "comments[\"body\"] = comments[\"body\"].fillna(\"\")\n",
    "comments[\"body_vader\"] = comments[\"body\"].map(minimal_clean_for_vader)\n",
    "comments[\"has_url\"] = comments[\"body\"].str.contains(r\"https?://\", na=False)\n",
    "comments[\"text_len\"] = comments[\"body\"].str.len().fillna(0)\n",
    "comments[\"author_deleted\"] = comments[\"author\"].isin([None, \"u/[deleted]\"])\n",
    "comments[\"is_bot\"] = comments[\"author\"].fillna(\"\").str.contains(\"automoderator\", case=False)\n",
    "\n",
    "# Eliminar duplicados y conservar índices limpios\n",
    "comments = comments.drop_duplicates(subset=[\"post_id\",\"comment_id\"]).reset_index(drop=True)\n",
    "posts = posts.drop_duplicates(subset=[\"post_id\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e704a",
   "metadata": {},
   "source": [
    "## Aplicar VADER y heurísticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64dd18cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VADER: 100%|██████████| 15131/15131 [00:01<00:00, 10443.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# Aplica VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# inicializa listas para scores\n",
    "v_neg, v_neu, v_pos, v_comp = [], [], [], []\n",
    "\n",
    "# Iterar y calcular scores\n",
    "for txt in tqdm(comments[\"body_vader\"].tolist(), desc=\"VADER\"):\n",
    "    scores = analyzer.polarity_scores(txt or \"\")\n",
    "    v_neg.append(scores.get(\"neg\", 0.0))\n",
    "    v_neu.append(scores.get(\"neu\", 0.0))\n",
    "    v_pos.append(scores.get(\"pos\", 0.0))\n",
    "    v_comp.append(scores.get(\"compound\", 0.0))\n",
    "\n",
    "# Asigna scores a DataFrame\n",
    "comments[\"vader_neg\"] = v_neg\n",
    "comments[\"vader_neu\"] = v_neu\n",
    "comments[\"vader_pos\"] = v_pos\n",
    "comments[\"vader_compound\"] = v_comp\n",
    "\n",
    "# Etiqueta a partir de compound (umbrales estándar)\n",
    "comments[\"sentiment_label\"] = pd.cut(\n",
    "    comments[\"vader_compound\"],\n",
    "    bins=[-1.0, -0.5, 0.5, 1.0],\n",
    "    labels=[\"neg\",\"neu\",\"pos\"],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Heurísticos acuerdo/desacuerdo\n",
    "comments[\"agrees\"] = comments[\"body_vader\"].map(lambda t: count_hits_regex(t, AGREE_TERMS))\n",
    "comments[\"disagrees\"] = comments[\"body_vader\"].map(lambda t: count_hits_regex(t, DISAGREE_TERMS))\n",
    "\n",
    "# Agregación por post\n",
    "agg = (\n",
    "    comments.groupby(\"post_id\", group_keys=False)\n",
    "    .apply(lambda g: pd.Series({\n",
    "        \"comments_total\": g[\"comment_id\"].count(),\n",
    "        \"comments_pos\": (g[\"sentiment_label\"] == \"pos\").sum(),\n",
    "        \"comments_neg\": (g[\"sentiment_label\"] == \"neg\").sum(),\n",
    "        \"comments_neu\": (g[\"sentiment_label\"] == \"neu\").sum(),\n",
    "        \"agree_hits\": g[\"agrees\"].sum(),\n",
    "        \"disagree_hits\": g[\"disagrees\"].sum(),\n",
    "    }), include_groups=False)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "# Calcula support_index con manejo de cero\n",
    "num = agg[\"comments_pos\"] + agg[\"agree_hits\"]\n",
    "denom = agg[\"comments_pos\"] + agg[\"comments_neg\"] + agg[\"agree_hits\"] + agg[\"disagree_hits\"]\n",
    "agg[\"support_index\"] = (num / denom.replace({0: pd.NA})).fillna(0.0).round(3)\n",
    "\n",
    "# Une a posts\n",
    "posts_proc = posts.merge(agg, on=\"post_id\", how=\"left\")\n",
    "for col in [\"comments_total\",\"comments_pos\",\"comments_neg\",\"comments_neu\",\"agree_hits\",\"disagree_hits\",\"support_index\"]:\n",
    "    if col not in posts_proc.columns:\n",
    "        posts_proc[col] = 0 if col != \"support_index\" else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be9c7c",
   "metadata": {},
   "source": [
    "## Guardado (CSV + JSONL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac80d24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listo.\n",
      "comments_with_vader: (15131, 18) → data\\enriched\\comments_with_vader.csv\n",
      "posts_with_support: (200, 18) → data\\enriched\\posts_with_support.csv\n"
     ]
    }
   ],
   "source": [
    "comments_out_csv = ENRICHED_DIR / \"comments_with_vader.csv\"\n",
    "comments_out_jsonl = ENRICHED_DIR / \"comments_with_vader.jsonl\"\n",
    "posts_out_csv = ENRICHED_DIR / \"posts_with_support.csv\"\n",
    "posts_out_jsonl = ENRICHED_DIR / \"posts_with_support.jsonl\"\n",
    "\n",
    "comments.to_csv(comments_out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "posts_proc.to_csv(posts_out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "with open(comments_out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in comments.iterrows():\n",
    "        f.write(json.dumps(row.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(posts_out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in posts_proc.iterrows():\n",
    "        f.write(json.dumps(row.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Listo.\")\n",
    "print(\"comments_with_vader:\", comments.shape, \"→\", comments_out_csv)\n",
    "print(\"posts_with_support:\", posts_proc.shape, \"→\", posts_out_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit-scrapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
